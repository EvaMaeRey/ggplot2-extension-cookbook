---
output: rmdformats::readthedown
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>", 
  eval = T,
  cache = T,
  fig.path = "man/figures/") 
```

# ggplot2-extension-cookbook

<!-- badges: start -->
<!-- badges: end -->

*The ggplot2 Extension Cookbook* aims to provide ggplot2 some extension strategies in a consistent and accessible way.

We group the content by extension type, provide demonstrations of their use, and then link (for now) to the code that defines them.  In that material, I'll try to stick to a formula to orient you to the ggplot2 extension:

- Step 0: get job done with 'base' ggplot2
- Step 1: Write a function for the 'compute'
- Step 2: Pass the compute to ggproto object
- Step 3: Pass ggproto to a user-facing function for use in a ggplot() pipeline
- Step 4: Try out/test/enjoy!

Regarding stat_'s vs. geom_'s, I take a geoms-first approach, because they are more commonly used.  I suspect this is they are more concrete descriptive of what the creator envisions for her plot, whereas stat_* may feel a be more 'adverbial' and nebulous in their description of rendered output. For example `ggplot(mtcars, aes(wt, mpg)) + stat_identity()` and
`ggplot(mtcars, aes(wt, mpg)) + geom_point()`  render identical ggplot objects, but the later might feel more descriptive of the resultant plot. Between these two options, the preference for the geom evident in the data too; on Github, there are 788 R language files containing 'stat_identity' whereas a staggering 261-thousand R language files contain 'geom_point'. 

Finally, most of the code is at the 'R for data science level, and not 'Advanced R' level, which hopefully will afford greater reach.  Object oriented programming (OOP) gets top billing in many extension materials, but many folks that are fluent in ggplot2 might not know much about OOP, in general, I try to see what can be accomplished while without much OOP and ggroto discussion.

*Currently, I'm experimenting with a ratio typology that you'll see in the section titles. The idea is to think about how the input data relates to the mark we see on the plot and in turn how the mark's information is stored in the the ggplot2 object. This is really new, and I'm unsure of how productive or precise it can be...*

# Preface and acknowledgements

I attended Thomas Lin Pederson's January 2020 talk 'Extending your ability to extend ggplot2' seated on the floor of a packed out ballroom.  The talk had the important central message - "you can be a ggplot2 extender".  And since then, I aimed to be an extender. I hope that this *ggplot2 Extension Cookbook* will help you on your extender journey and, especially if you are fluent in R and ggplot2, it says to you "you can be a ggplot2 extender".

I've been using ggplot2 since about 2017, and really enjoyed the user interface.  In general, the syntax does a fantastic job at letting users compose their plots bit-by-bit, closely resembling to how you might sketch out a plot on a notepad or blackboard, and describing your decisions as you go to yourself or a colleague.  Or as Thomas Lin Pederson has also said, 'ggplot2 lets you *speak your plot into existence*'.  And perhaps a little less eloquently by Hadley Wickham's, the ggplot2 author,  "This is what I'm thinking; your the computer, now go and do it!" (my paraphrase of the author talking about how he thought data viz should feel as a statistical consultant -- well before ggplot2 existed).

But there were pain points when using 'base' ggplot2; for me, this was mostly when a geom didn't exist for doing some compute in the background, but maybe wanted that compute done over and over, and it would be a slough to do it over and over upstream to the plotting environment. This pre-computation problem felt manageable in classroom setting which I was in through early 2020 but when I moved to a primarily analytic role at West Point --- where the volume of analysis was simply higher and turn around times faster --- I felt the problem much more acutely.  (Overnight, I went from weak preference for geom_col - to strong preference for geom_bar!)   Extension seemed to offer the solution to the problem and I was more motivated in my analyst role.

However, I experienced about a year of failure and frustration when first entering the extension space.  If I weren't so convinced of the efficiency gains that it would eventually yield, I'd likely have given up.  Recognizing the substancial hurdles for even long time R and ggplot2 users, I think there is space for more ggplot2 extension reference materials, such as the recipes in the *ggplot2 Extension Cookbook*.

I'm grateful for several expediences and the efforts of others that have refined these new materials.  First, just after getting my own feet wet in extension, I had the chance to work on extension with students in the context of independent studies.  Our focus was the same type of extension that Pederson demonstrated -- a geom that used a Stat to do some computational work, and then inherited the rest of its behavior from a more primitive geom.  

Working with first and second year undergrad students meant that I had to think about and formulate workflow - where reference material would even be accessibility to very new R users. As veterans of just one or two stats classes that used R and ggplot2, what would they find familiar and accessible?  What might we be able to de-emphasize and still succeed?  I'm grateful to these  This worked with students and I had a chance to refine it further. 

The following steps emerged and persisted into our tutorial formula:

- Step 0: get job done with 'base' ggplot2
- Step 1: Write a function for the 'compute'
- Step 2: Pass the compute to ggproto
- Step 3: Pass ggproto to stat/geom/facet function
- Step 4: Try out/test/enjoy!

Taking on new R users to try to do something only just felt like I was wrapping my head around was a leap of faith.  I was very impressed with what the students were able to accomplish during a single semester, and breathed a sign of relief. 

I wondered how the strategy would perform with experienced R and ggplot2 users.  Being an academic, I wanted to  assess further and I went down the route of devising a tutorial [with assistance from independent study student Morgan Brown] and formally getting feedback on it via focus groups and a survey, after refining the tutorial.  

I did some research on ggplot2 extension among ggplot2 and R 'super users' and have found that the perhaps this community is under-served, but with the right materials, more folks could get into ggplot2 extension.

I fielded 'easy geom recipes' with a group of statistics educators, conducting a survey on the resource and also getting feedback via a focus group.

Among my favorite quotation from the focus groups is something that validated the efforts but also challenged me:

> it was ... easy. And I felt empowered as a result of that.... But you know, like, my problem isn't gonna be that easy.

To that participant, I'd say 'Sometimes it *is* that easy'.  But he is right, that often times I come to an extension problem and am suprised that the strategy that I think is going to work doesn't, or at least not without a little fiddling.   

The feedback on the easy-geom-recipes was collected in March 2023. 

So, I'm interested in simply developing new, useful extensions. 

To try to make those experiences valuable to others, I follow the 'recipe' formula as much as possible so that as strategies morph, one still recognizes where we are at a high-level in the process.

Now to serve the community and for use in this *ggplot2 Extension Cookbook*, I'm using an in-README documentation strategy for my ggplot2 extension packages so that the development stories are in one place instead of scattered between package files. 

This is made possible via the readme2pkg R package and in turn the knitr package written by Yihui Xie, whose tools (literate programming, xaringan and other tools) had already played a huge role in my relationship with ggplot2.  Before arriving at this solution, I felt that I'd need to divide my energy between moving forward in my own extension development journey and educator.  I'm grateful that with the magic of knitr::knitr_code$get() I allow code to sit in the README narrative and also send it to the required .R directory to serve in the package. 

```{r, echo = F, eval = T}
readlines_wo_roxygen <- function(x = "../../ggwaterfall/R/StatWaterfall.R"){
  
  lines <- readLines(x) 
  lines[!stringr::str_detect(lines, "^#'")]
  
}

```

At present, I'll just show examples of functionality, and then link to the READMEs for further investigation of the specific recipes/strategies used. 

I'm personally grateful to other ggplot2 extenders and R enthusiasts that have supported this journey.

I'm also grateful to the ggplot2 development team .

I'm also indebted to my Department of Mathematics and Dean Data Cell colleagues at West Point, for sitting through some talks (some extemporaneous and muddled) where I tried to articulate my ggplot2 extension dreams.

Finally, to Winston Chang, who gets top billing in the ggplot2 extension vignette along with your ggproto, I hope you won't mind the general approach here which experiments with making ggproto as ignorable as possible for OOP noobs.  I also hope to meet you someday and hear more about the early days of ggproto, maybe at ggplot2 extenders meetup as a special guest, perhaps January 2025.

And finally, finally to Hadley Wickham and Leland Wilkinson having incredible insights and acting on them.  


# Getting started

For best results, I'd recommend *diving* in by actually creating some geoms as prompted in the 'easy geom recipes' tutorial.  Having completed these exercises, you'll have lived the geom creation from start to finish, will be well oriented to the consistent steps in the process for the patterns used in what follows. 

# geom_*: writing new definitions for where and how of marks on ggplots

This section tackles creating new geom_* layers.  The strategy is to look at compute that you'd do without extension capabilities (Step 0), and then create a Stat for that (Step 1 & 2), and then compose a user-facing function, which inherits other behavior from a more primitive geom (Step 3), so that ggplot2 can do compute for you in the background (Step 4).

## geom_text_coordinate: **1:1:1** 

- for each row in the input dataframe ...
- we'll perceive a single mark
- which will be defined by as single row in the internal dataframe

```{r}
library(tidyverse)
library(ggxmean)

cars %>% 
  mutate(coords = 
           paste0("(", speed, ",", dist, ")")) |>
  ggplot() + 
  aes(x = speed, y = dist) + 
  geom_point() + 
  geom_text(aes(label = coords), 
            check_overlap = T,
            hjust = 0,
            vjust = 0)
```

But we'd like the syntax...

```{r, eval = F}
ggplot(cars) + 
  aes(x = speed, y = dist) + 
  geom_point() + 
  geom_text_coordinate(
            check_overlap = T,
            hjust = 0,
            vjust = 0)
```


### Step 1: compute

```{r}
compute_group_coordinates <- function(data, scales) {

data |>
    dplyr::mutate(label = 
                    paste0("(", data$x, ", ", 
                           data$y, ")"))
}

cars %>% 
  rename(x = speed, y = dist) %>% 
  compute_group_coordinates() %>% 
  head()
```

# Step 2: pass to ggproto object

```{r}
StatCoordinate <- ggplot2::ggproto(`_class` = "StatCoordinate",
                 `_inherit` = ggplot2::Stat,
                 required_aes = c("x", "y"),
                 compute_group = compute_group_coordinates)

```

Step 3. Write user facing function.

```{r}
# part a. use ggplot2::layer which requires specifying Geom and Stat
ggplot(data = cars) + 
  aes(x = speed, y = dist) + 
  geom_point() + 
  ggplot2::layer(
    stat = StatCoordinate,
    geom = ggplot2::GeomText,
    position = "identity"
    )

# part b. create geom_* user-facing function using g
geom_text_coordinate <- function(mapping = NULL, 
                                 data = NULL,
                                 position = "identity", 
                                 show.legend = NA,
                                 inherit.aes = TRUE, 
                                 na.rm = FALSE,
                                 ...) {
  ggplot2::layer(
    stat = StatCoordinate,
    geom = ggplot2::GeomText, 
    mapping = mapping,
    data = data,
    position = position,
    show.legend = show.legend,
    inherit.aes = inherit.aes,
    params = list(na.rm = na.rm, ...)
  )
}
```


```{r}


```



## geom_xy_means: **n:1:1** 

*many rows from a dataset: will be summarized and visualized by as single mark: the mark will be defined by one row of data*

```{r}
ggplot(mtcars) + 
  aes(x = wt, y = mpg) + 
  geom_point() + 
  geom_xy_means(size = 8)

last_plot() + 
  aes(color = as.factor(am))
```

#### build

```{r, code = readLines("../../ggxmean/R/geom_xy_means.R")}

```



## geom_chull: **n:1:k**

rewriting a classic from the ggplot2 vignette...

```{r}
library(tidyverse)
chull_row_ids <- chull(mtcars$wt, mtcars$mpg)
chull_row_ids
mtcars_chull_subset <- mtcars %>% slice(chull_row_ids)

ggplot(mtcars) + 
  aes(x = wt, y = mpg) + 
  geom_point() + 
  geom_polygon(data = mtcars_chull_subset, 
               alpha = .3, 
               color = "black")
```


### Step 1 and 2

```{r}
# Step 1
compute_group_c_hull <- function(data, scales){
  
  chull_row_ids <- chull(data$x, data$y)
  
  data %>% slice(chull_row_ids)
  
}
```

Below, we see that the dataset is reduced to 11 rows which constitute the convex hull perimiter.  

```{r}
mtcars %>% # 32 rows
  rename(x = wt, y = mpg) %>% 
  compute_group_c_hull() # 11 rows
```

```{r}
# Step 2
StatChull <- ggproto(`_class` = "StatChull",
                     `_inherit` = ggplot2::Stat,
                     compute_group = compute_group_c_hull,
                     required_aes = c("x", "y"))
```




### Step 3. Write user-facing geom_/stat_ Function(s)

```{r}
geom_chull <- function(mapping = NULL, 
                        data = NULL,
                        position = "identity", 
                        na.rm = FALSE, 
                        show.legend = NA,
                        inherit.aes = TRUE, ...) {

  ggplot2::layer(
    stat = StatChull, 
    geom = ggplot2::GeomPolygon, 
    data = data, mapping = mapping,
    position = position, 
    show.legend = show.legend, 
    inherit.aes = inherit.aes,
    params = list(na.rm = na.rm, ...)
  )

}
```

### Step 4. Try out/test/ enjoy

```{r}
ggplot(data = mtcars) + 
  aes(x = wt, y = mpg) + 
  geom_point() + 
  geom_chull(alpha = .3)

last_plot() + 
  aes(color = factor(am),
      fill = factor(am))
```

---


## geom_circle: **1:1:n** 

*a single row in a dataframe: will be visualized by a single mark: the mark will be defined by many-row in an internal dataframe*

for each row in the dataframe, a single geometry is visualized, but each geom is defined by many rows...

## geom_us_state: **1:1:n**


```{r}
```


## geom_ggcirclepack: **1:1:n, interdependance**

*a many-row geom for each row of the input data frame, with interdependence between input observations.*

```{r}
```


## geom_ols: **n:k:w; interdependence**

*between-group computation*

```{r}
```




## geom_county: **1:1:1 via geometry sf**

*a geom defined by an sf geometry column*

```{r}
```


## geom_candlestick summarize first, then interdependence ...

```{r}
```

## geom_pie: **n->1:1:1**


## geom_wedge: **n->1:1:n**





# stat_* layers: keeping flexible via stat_* functions


## stat_chull 

Rather than defining geom functions, you might instead write stat_* functions which can be used with a variety of geoms.  Let's contrast geom_chull and stat_chull below.

```{r}
geom_chull <- function(mapping = NULL, 
                        data = NULL,
                        position = "identity", 
                        na.rm = FALSE, 
                        show.legend = NA,
                        inherit.aes = TRUE, ...) {

  ggplot2::layer(
    stat = StatChull, 
    geom = ggplot2::GeomPolygon, 
    data = data, mapping = mapping,
    position = position, 
    show.legend = show.legend, 
    inherit.aes = inherit.aes,
    params = list(na.rm = na.rm, ...)
  )

}


stat_chull <- function(mapping = NULL, 
                       geom = ggplot2::GeomPolygon, 
                       data = NULL,
                       position = "identity", 
                       na.rm = FALSE, 
                       show.legend = NA,
                       inherit.aes = TRUE, ...) {

  ggplot2::layer(
    stat = StatChull, 
    geom = geom, 
    data = data, 
    mapping = mapping,
    position = position, 
    show.legend = show.legend, 
    inherit.aes = inherit.aes,
    params = list(na.rm = na.rm, ...)
  )

}



```

The construction is almost identical.  However, in the stat version, the geom is flexible because it can be user defined, instead of being hard-coded in the function.  Its use allows you to go in different visual directions, but might have a higher cognative load.


```{r}
p <- ggplot(data = mtcars) + 
  aes(x = wt, y = mpg) + 
  geom_point() 

p +
  stat_chull(alpha = .3)

p +
  stat_chull(geom = "point",
             color = "red",
             size = 4)

p + 
  stat_chull(geom = "text",
             label = "c-hull point",
             hjust = 0)

# shows stat does not well-serve "path" geom
p + 
  stat_chull(geom = "path",
             label = "c-hull point",
             hjust = 0)
```

## stat_waterfall: **1:1:1; interdependence**

Because the stat_* functions might require more cognitively from the user, aliasing might be a good idea, creating one or more geoms_* the stat layer.

The function geom_waterfall() is exported from the ggwaterfall package, which is being developed at github.com/EvaMaeRey/ggwaterfall

*One-row geom for each row in input dataset; geom interdependence*

A waterfall plot displays inflows and outflows that occur as a result of events as well as the balance across multiple events. It is typically displayed as a series of rectangles.  Because the net change is displayed (cumulative change), there is interdependence between the geometries on our plot -- where one rectangle ends, the next in the series begins. 

In this example we'll see how to alias the stat to a geom user-facing function (stat_waterfall -> geom_waterfall), and also how to change the geom to allow for additional convenient user-facing functions (stat_waterfall -> geom_waterfall_label).  We prep to create geom_waterfall label by using the default_aes slot in in the ggproto step.  

```{r, eval = T}
library(ggwaterfall)
library(tidyverse)
flow_df <- data.frame(event = c(
                     "Sales", 
                     "Refunds",
                     "Payouts", 
                     "Court Losses", 
                     "Court Wins", 
                     "Contracts", 
                     "Fees"),
           change = c(6400, -1100, 
                      -100, -4200, 3800, 
                      1400, -2800)) |> 
  mutate(event = factor(event))

flow_df

flow_df |> 
  ggplot() +
  geom_hline(yintercept = 0) +
  aes(change = change, 
      x = event) + # event in order
  geom_waterfall() + 
  geom_waterfall_label() + 
  scale_y_continuous(expand = expansion(.1)) + 
  scale_fill_manual(values = c("springgreen4", "darkred"))
```

The strategy to create geom waterfall follows the standard four steps. 

### Step 0

For 'step 0', we base ggplot2 to accomplish this task, and actually pretty closely follow Hadley Wickham's short paper that tackles a waterfall plot with ggplot2. https://vita.had.co.nz/papers/ggplot2-wires.pdf

### Steps 1 and 2


Then, we bundle up this computation into a function (step 1), called compute_panel_waterfall.  This function then define the compute_panel element in the ggproto object (step 2).  We want the computation done *panel-wise* because of the interdependence between the events, which run along the x axis.  Group-wise computation (the defining compute_group element), would fail us, as the cross-event interdependence would not be preserved. 

```{r, code = readlines_wo_roxygen("../../ggwaterfall/R/StatWaterfall.R")}
```

### Step 3

In step 3, we define stat_waterfall, passing along StatWaterfall to create a ggplot2 layer function. We include a standard set of arguments, and we set the geom to ggplot2::GeomRect.  

```{r, code = readlines_wo_roxygen("../../ggwaterfall/R/geom_waterfall.R")}
```

### Step 4

In Step 4, we get to try out the functionality.  

```{r, out.width="33%", fig.show='hold'}
flow_df |> 
  ggplot() +
  geom_hline(yintercept = 0) +
  aes(change = change, 
      x = event) + # event in order
  geom_waterfall() + 
  geom_waterfall_label() + 
  scale_y_continuous(expand = expansion(.1)) + 
  scale_fill_manual(values = c("springgreen4", "darkred"))

last_plot() + 
  aes(x = fct_reorder(event, change))

last_plot() + 
  aes(x = fct_reorder(event, abs(change)))
```

The final plot shows that while there are some convenience defaults for label and fill, these can be over-ridden. 

```{r}
last_plot() + 
  aes(label = ifelse(change > 0, "gain", "loss")) + 
  aes(fill = NULL)
```


# borrowing compute

geom_smoothfit: **1:1:1** highjacking (?) an existing stats's computational powers

## geom_smoothfit: **1:1:1** ggproto piggybacking on compute...

n:1:80 is geom_smooth default.

```{r}
library(ggsmoothfit)
```

# add default aesthetics

## geom_barlab:  Adding defaults to existing stats via ggproto editing

```{r}

```




# modified start points; ggverbatim(), 

## ggverbatim()

```{r}
library(ggverbatim)
```

# ggedgelist()

```{r}
library(ggedgelist)
```




# defining theme...

```{r}
#library(ggchalkboard)
```

# geom-led extension

## ggscatterplot

```{r}

```

# wrapping fiddly functions (annotate and theme)

```{r}
library(ggstamp)
```

```{r}
#library(ggcons) # 
```


# ggtedious *formal testing*

```{r}
#library(ggtedius)
```
