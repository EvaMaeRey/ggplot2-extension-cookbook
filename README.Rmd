---
output: 
  github_document:
    toc: TRUE
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = F,
  fig.path = "man/figures/")

```

# ggplot2-extension-cookbook

<!-- badges: start -->
<!-- badges: end -->

The goal of ggplot2-extension-cookbook is to collect some strategies for ggplot2 extension.  

Since I've attended the talk 'Extending your ability to extend ggplot2' in Jan 2020, I've been interested in extension.  

For me, it took a while to get up and running with the existing materials. 

However, I had the chance to work on extension with students meant that I had to formulate a workflow.  This worked with students and I had a chance to refine it further. 

Step 0: get job done with 'base' ggplot2
Step 1: Write a function for the 'compute'
Step 2: Pass the compute to ggproto
Step 3: Pass ggproto to stat/geom/facet function
Step 4: Try out/test/enjoy!

After working with students, I did some research on ggplot2 extension among ggplot2 and R 'super users' and have found that the perhaps this community is under-served, but with the right materials, more folks could get into ggplot2 extension.

I fielded 'easy geom recipes' with a group of statistics educators, conducting a survey on the resource and also getting feedback via a focus group.

Meanwhile, I'm interested in simply developing new, useful extensions. 

To try to make those experiences valuable to others, I follow the 'recipe' formula as much as possible so that as strategies morph, one still knows where one is in the process.

Moreover, I'm using an in-readme documentation strategy for my development packages so that the development stories are documented in one place. This is made possible via the readme2pkg R package and in turn the knitr package written by Yihui Xie, whose tools (literate programming, xaringan and other tools) have changed my relationship with ggplot2. 

At present, I'll just show examples of functionality, and then link to the READMEs for further investigation of the specific recipes/strategies used. 


# one geom per row in a input data frame

```{r}
ggxmean::geom_text_coordinate()
```

Note: in this situation, you can also get away with delayed aesthetic evaluation...


# one geom summarizing many rows in an input data frame, group-wise computation

```{r}
ggxmean::geom_xy_means()
```

# a geom defined by many rows, generated by a single row of the input data frame




```{r}
library(ggcircle)
library(ggcirclepack)
```


# between-group computation

```{r}
library(ggols)
```


## a geom defined by an sf geometry column

```{r}
library(ggnorthcarolina)
```


# One geom per row, but interdependence (waterfall)

```{r}
library(ggwaterfall)
```

# summarize first, then interdependence ...

```{r}
library(ggcandlestick)
```

# Piggybacking on existing stats

```{r}
library(ggsmoothfit)
```

# Adding defaults to existing stats via ggproto editing

```{r}

```


# modifying ggplot() function (data transformation)

```{r}
library(ggverbatim)
```


# modifying ggraph() function

```{r}
library(ggedgelist)
```


# wrapping fiddly functions (annotate and theme)

```{r}
library(ggstamp)
```

```{r}
library(ggcons) # 
```

# defining theme...

```{r}
library(ggchalkboard)
```

# What about formal testing?

```{r}
library(ggtedius)
```
